{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef478a4-0e2d-4067-819e-f22ef7486e84",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a43625-b6e9-47f1-8260-0506c8218832",
   "metadata": {},
   "source": [
    "# 1. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d16599-ed91-4c2b-b038-5433ad35070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Setup\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab31a1dc-4068-441b-9cf3-3f6a9a9bf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba028a05-550e-4fee-9a2f-f0b2e860d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Step 2: Load your CSV file\n",
    "df = pd.read_csv(\"whisper_with_groundtruth_100.csv\")\n",
    "\n",
    "# Step 3: Auto-label with spaCy NER\n",
    "def extract_entities(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "df[\"entities\"] = df[\"whisper_transcript\"].astype(str).apply(extract_entities)\n",
    "\n",
    "# Step 4: Convert to training format for inspection\n",
    "train_data = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row[\"whisper_transcript\"]\n",
    "    entities = row[\"entities\"]\n",
    "    if entities:\n",
    "        train_data.append({\"text\": text, \"annotations\": {\"entities\": entities}})\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "ner_df = pd.DataFrame(train_data)\n",
    "ner_df.to_csv(\"ner_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596cb17f-0fda-42f1-9bf1-88ed431da198",
   "metadata": {},
   "source": [
    "# 2. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52e5da1-06ea-48d5-943a-bcd6f236e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a2e0ca-ad95-4b0b-a0d7-02be6b788b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the CSV file\n",
    "df = pd.read_csv(\"ner_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929eea6e-d627-4754-9e38-86e3ab697fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Row 1 ---\n",
      "Original Text:\n",
      "vivid light of a judgment day. The girl, moreover, was not prone to take for granted that she herself lived in the mind of others.\n",
      "Entities:\n",
      "DATE: 'a judgment day'\n",
      "\n",
      "--- Row 2 ---\n",
      "Original Text:\n",
      "asked Isabella Breply, why, as a kind of compliment, a compliment on what? On your so beautifully existing. He liked me too much, she presently declared. That's a way we all have.\n",
      "Entities:\n",
      "PERSON: 'Isabella Breply'\n",
      "\n",
      "--- Row 3 ---\n",
      "Original Text:\n",
      "Henrietta doesn't. Oh, hang Henrietta, said Ralph Corsley. If you ask me, I'm delighted at it. Is that why your father did it for your amusement? I differ with Miss Stackpole.\n",
      "Entities:\n",
      "ORG: 'Henrietta'\n",
      "ORG: 'Henrietta'\n",
      "PERSON: 'Ralph Corsley'\n",
      "PERSON: 'Stackpole'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the CSV file\n",
    "df = pd.read_csv(\"ner_train_data.csv\")\n",
    "\n",
    "# Step 2: Parse stringified lists into Python objects\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Print entity spans and labels for the first 10 rows\n",
    "for i in range(min(3, len(df))):\n",
    "    row = df.iloc[i]\n",
    "    text = row[\"text\"]\n",
    "    entities = row[\"annotations\"][\"entities\"]\n",
    "\n",
    "    print(f\"\\n--- Row {i+1} ---\")\n",
    "    print(\"Original Text:\")\n",
    "    print(text)\n",
    "    print(\"Entities:\")\n",
    "    for start, end, label in entities:\n",
    "        print(f\"{label}: '{text[start:end]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cba0f3-0904-4826-bef6-f9a07980650a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cebd28a4-6e22-4064-9e81-11f7606efa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 583it [11:12,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 1: {'ner': np.float32(38593.55)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 583it [11:10,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 2: {'ner': np.float32(30071.406)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 583it [11:17,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 3: {'ner': np.float32(26914.508)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 583it [10:46,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 4: {'ner': np.float32(25240.418)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 583it [09:57,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 7: {'ner': np.float32(24523.87)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 583it [10:07,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 8: {'ner': np.float32(24345.117)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 583it [10:07,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 9: {'ner': np.float32(24128.111)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 583it [10:01,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 10: {'ner': np.float32(23705.365)}\n",
      "Model saved to: ner_custom_model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load CSV and parse annotations\n",
    "df = pd.read_csv(\"ner_train_data.csv\")\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(ast.literal_eval)\n",
    "\n",
    "# Step 2: Format into spaCy training format\n",
    "TRAIN_DATA = []\n",
    "for i, row in df.iterrows():\n",
    "    TRAIN_DATA.append((row[\"text\"], row[\"annotations\"]))\n",
    "\n",
    "# Step 3: Create blank English NLP pipeline and add NER\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Step 4: Add entity labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for start, end, label in annotations[\"entities\"]:\n",
    "        ner.add_label(label)\n",
    "\n",
    "# Step 5: Train the model\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):  # number of epochs\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.5))\n",
    "        for batch in tqdm(batches, desc=f\"Epoch {i+1}\"):\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update([example], drop=0.3, losses=losses)\n",
    "        print(f\"Losses at epoch {i+1}:\", losses)\n",
    "\n",
    "# Step 6: Save the trained model\n",
    "output_dir = \"ner_custom_model\"\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d25da-078c-4d09-b117-471f4bbe9027",
   "metadata": {},
   "source": [
    "# Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28194d53-d6a5-482c-b084-412c6092f27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in Text:\n",
      "Grace PERSON\n",
      "Northwestern University FAC\n",
      "June 2024 DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load your trained model\n",
    "nlp2 = spacy.load(\"ner_custom_model\")\n",
    "\n",
    "# Run it on new text\n",
    "text = \"Grace is a student registed in Northwestern University on June 2024.\"\n",
    "doc = nlp2(text)\n",
    "\n",
    "# Print recognized entities\n",
    "print(\"Entities in Text:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb68f0-06ce-4b4c-976d-4d3820e40951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
